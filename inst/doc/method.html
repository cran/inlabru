<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Iterative INLA method</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Iterative INLA method</h1>



<div id="the-inla-method-for-linear-predictors" class="section level2">
<h2>The INLA method for linear predictors</h2>
<p>The INLA method is used to compute fast approximative posterior
distribution for Bayesian generalised additive models. The hierarchical
structure of such a model with latent Gaussian components <span class="math inline">\(\boldsymbol{u}\)</span>, covariance parameters
<span class="math inline">\(\boldsymbol{\theta}\)</span>, and measured
response variables <span class="math inline">\(\boldsymbol{y}\)</span>,
can be written as <span class="math display">\[
\begin{aligned}
\boldsymbol{\theta} &amp;\sim p(\boldsymbol{\theta}) \\
\boldsymbol{u}|\boldsymbol{\theta} &amp;\sim
\mathcal{N}\!\left(\boldsymbol{\mu}_u,
\boldsymbol{Q}(\boldsymbol{\theta})^{-1}\right) \\
\boldsymbol{\eta}(\boldsymbol{u}) &amp;= \boldsymbol{A}\boldsymbol{u} \\
\boldsymbol{y}|\boldsymbol{u},\boldsymbol{\theta} &amp; \sim
p(\boldsymbol{y}|\boldsymbol{\eta}(\boldsymbol{u}),\boldsymbol{\theta})
\end{aligned}
\]</span> where typically each linear predictor element, <span class="math inline">\(\eta_i(\boldsymbol{u})\)</span>, is linked to a
location parameter of the distribution for observation <span class="math inline">\(y_i\)</span>, for each <span class="math inline">\(i\)</span>, via a (non-linear) link function <span class="math inline">\(g^{-1}(\cdot)\)</span>. In the R-INLA
implementation, the observations are assumed to be conditionally
independent, given <span class="math inline">\(\boldsymbol{\eta}\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
</div>
<div id="approximate-inla-for-non-linear-predictors" class="section level2">
<h2>Approximate INLA for non-linear predictors</h2>
<p>The premise for the inlabru method for non-linear predictors is to
build on the existing implementation, and only add a linearisation step.
The properties of the resulting approximation will depend on the nature
of the non-linearity.</p>
<p>Let <span class="math inline">\(\widetilde{\boldsymbol{\eta}}(\boldsymbol{u})\)</span>
be a non-linear predictor, and let <span class="math inline">\(\overline{\boldsymbol{\eta}}(\boldsymbol{u})\)</span>
be the 1st order Taylor approximation at <span class="math inline">\(\boldsymbol{u}_0\)</span>, <span class="math display">\[
\overline{\boldsymbol{\eta}}(\boldsymbol{u})
= \widetilde{\boldsymbol{\eta}}(\boldsymbol{u}_0) +
\boldsymbol{B}(\boldsymbol{u} - \boldsymbol{u}_0)
= \left[\widetilde{\boldsymbol{\eta}}(\boldsymbol{u}_0) -
\boldsymbol{B}\boldsymbol{u}_0\right] + \boldsymbol{B}\boldsymbol{u}
,
\]</span> where <span class="math inline">\(\boldsymbol{B}\)</span> is
the derivative matrix for the non-linear predictor, evaluated at <span class="math inline">\(\boldsymbol{u}_0\)</span>.</p>
<p>The non-linear observation model <span class="math inline">\(p(\boldsymbol{y}|g^{-1}[\widetilde{\boldsymbol{\eta}}(\boldsymbol{u})],\boldsymbol{\theta})\)</span>
is approximated by replacing the non-linear predictor with its
linearisation, so that the linearised model is defined by <span class="math display">\[
\overline{p}(\boldsymbol{y}|\boldsymbol{u},\boldsymbol{\theta})
=
p(\boldsymbol{y}|\overline{\boldsymbol{\eta}}(\boldsymbol{u}),\boldsymbol{\theta})
=
p(\boldsymbol{y}|g^{-1}[\overline{\boldsymbol{\eta}}(\boldsymbol{u})],\boldsymbol{\theta})
\approx
p(\boldsymbol{y}|g^{-1}[\widetilde{\boldsymbol{\eta}}(\boldsymbol{u})],\boldsymbol{\theta})
=
p(\boldsymbol{y}|\widetilde{\boldsymbol{\eta}}(\boldsymbol{u}),\boldsymbol{\theta})
=
\widetilde{p}(\boldsymbol{y}|\boldsymbol{u},\boldsymbol{\theta})
\]</span> The non-linear model posterior is factorised as <span class="math display">\[
\widetilde{p}(\boldsymbol{\theta},\boldsymbol{u}|\boldsymbol{y}) =
\widetilde{p}(\boldsymbol{\theta}|\boldsymbol{y})\widetilde{p}(\boldsymbol{u}|\boldsymbol{y},\boldsymbol{\theta}),
\]</span> and the linear model approximation is factorised as <span class="math display">\[
\overline{p}(\boldsymbol{\theta},\boldsymbol{u}|\boldsymbol{y}) =
\overline{p}(\boldsymbol{\theta}|\boldsymbol{y})\overline{p}(\boldsymbol{u}|\boldsymbol{y},\boldsymbol{\theta})
.
\]</span></p>
<div id="fixed-point-iteration" class="section level3">
<h3>Fixed point iteration</h3>
<p>The remaining step of the approximation is how to choose the
linearisation point <span class="math inline">\(\boldsymbol{u}_0\)</span>. We start by introducing
a functional <span class="math inline">\(f(\overline{p}_{\boldsymbol{v}})\)</span> of the
posterior distribution linearised at <span class="math inline">\(\boldsymbol{v}\)</span>, that generates a latent
field configuration. We then seek a fix point of the functional, so that
<span class="math inline">\(\boldsymbol{u}_0=f(\overline{p}_{\boldsymbol{u}_0})\)</span>.
Potential choices for <span class="math inline">\(f(\cdot)\)</span>
include the posterior expectation <span class="math inline">\(\overline{E}(\boldsymbol{u}|\boldsymbol{y})\)</span>
and the joint conditional mode, <span class="math display">\[
f(\overline{p}_{\boldsymbol{v}})=\mathop{\mathrm{arg\,max}}_{\boldsymbol{u}}
\overline{p}_{\boldsymbol{v}}(\boldsymbol{u}|\boldsymbol{y},\widehat{\boldsymbol{\theta}}),
\]</span> where <span class="math inline">\(\widehat{\boldsymbol{\theta}}=\mathop{\mathrm{arg\,max}}_{\boldsymbol{\theta}}
\overline{p}_{\boldsymbol{v}}(\boldsymbol{\theta}|\boldsymbol{y})\)</span>
(used in <code>inlabru</code> from version 2.2.3<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>) One key to the fix
point iteration is that the observation model is linked to <span class="math inline">\(\boldsymbol{u}\)</span> only through the
non-linear predictor <span class="math inline">\(\widetilde{\boldsymbol{\eta}}(\boldsymbol{u})\)</span>.</p>
<ol start="0" style="list-style-type: decimal">
<li>Let <span class="math inline">\(\boldsymbol{u}_0\)</span> be an
initial linearisation point for the latent variables.</li>
<li>Compute the predictor linearisation at <span class="math inline">\(\boldsymbol{u}_0\)</span>,</li>
<li>Compute the linearised INLA posterior <span class="math inline">\(\overline{p}(\boldsymbol{\theta}|\boldsymbol{y})\)</span></li>
<li>Let <span class="math inline">\(\boldsymbol{u}_1=f(\overline{p}_{\boldsymbol{u}_0})\)</span>
be the initial candidate for new linearisation point.</li>
<li>Let <span class="math inline">\(\boldsymbol{u}_\alpha=(1-\alpha)\boldsymbol{u}_0+\alpha\boldsymbol{u}_1\)</span>,
and find the value <span class="math inline">\(\alpha\)</span> that
minimises <span class="math inline">\(\|\widetilde{\eta}(\boldsymbol{u}_\alpha)-\overline{\eta}(\boldsymbol{u}_1)\|\)</span>.</li>
<li>Set the linearisation point equal to <span class="math inline">\(\boldsymbol{u}_\alpha\)</span> and repeat from
step 1, unless the iteration has converged to a given tolerance.</li>
</ol>
<div id="line-search" class="section level4">
<h4>Line search</h4>
<p>In step 4, an approximate line search can be used, that avoids many
potentially expensive evaluations of the non-linear predictor. We
evaluate <span class="math inline">\(\widetilde{\boldsymbol{\eta}}_1=\widetilde{\boldsymbol{\eta}}(\boldsymbol{u}_1)\)</span>
and make use of the linearised predictor information. Let <span class="math inline">\(\widetilde{\boldsymbol{\eta}}_\alpha=\widetilde{\boldsymbol{\eta}}(\boldsymbol{u}_\alpha)\)</span>
and <span class="math inline">\(\overline{\boldsymbol{\eta}}_\alpha=\overline{\boldsymbol{\eta}}(\boldsymbol{u}_\alpha)=(1-\alpha)\widetilde{\boldsymbol{\eta}}(\boldsymbol{u}_0)+\alpha\overline{\boldsymbol{\eta}}(\boldsymbol{u}_1)\)</span>.
An exact line search would minimise <span class="math inline">\(\|\widetilde{\boldsymbol{\eta}}_\alpha-\overline{\boldsymbol{\eta}}_1\|\)</span>.
Instead, we define a quadratic approximation to the non-linear predictor
as a function of <span class="math inline">\(\alpha\)</span>, <span class="math display">\[
\breve{\boldsymbol{\eta}}_\alpha =
\overline{\boldsymbol{\eta}}_\alpha + \alpha^2
(\widetilde{\boldsymbol{\eta}}_1 - \overline{\boldsymbol{\eta}}_1)
\]</span> and minimise the quartic polynomial in <span class="math inline">\(\alpha\)</span>, <span class="math display">\[
\begin{aligned}
\|\breve{\boldsymbol{\eta}}_\alpha-\overline{\boldsymbol{\eta}}_1\|^2
&amp;=
\| (\alpha-1)(\overline{\boldsymbol{\eta}}_1 -
\overline{\boldsymbol{\eta}}_0) + \alpha^2
(\widetilde{\boldsymbol{\eta}}_1 - \overline{\boldsymbol{\eta}}_1) \|^2
.
\end{aligned}
\]</span> If initial expansion and contraction steps are carried out,
leading to an initial guess of <span class="math inline">\(\alpha=\gamma^k\)</span>, where <span class="math inline">\(\gamma&gt;1\)</span> is a scaling factor (see
<code>?bru_options</code>, <code>bru_method$factor</code>) and <span class="math inline">\(k\)</span> is the (signed) number of expansions
and contractions, the quadratic expression is replaced by <span class="math display">\[
\begin{aligned}
\|\breve{\boldsymbol{\eta}}_\alpha-\overline{\boldsymbol{\eta}}_1\|^2
&amp;=
\| (\alpha-1)(\overline{\boldsymbol{\eta}}_1 -
\overline{\boldsymbol{\eta}}_0) + \frac{\alpha^2}{\gamma^{2k}}
(\widetilde{\boldsymbol{\eta}}_{\gamma^k} -
\overline{\boldsymbol{\eta}}_{\gamma^k}) \|^2
,
\end{aligned}
\]</span> which is minimised on the interval <span class="math inline">\(\alpha\in[\gamma^{k-1},\gamma^{k+1}]\)</span>.</p>
<p>A potential improvement of step 4 might be to also take into account
the prior distribution for <span class="math inline">\(\boldsymbol{u}\)</span> as a minimisation penalty,
to avoid moving further than would be indicated by a full likelihood
optimisation.</p>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p> Note: In <code>inlabru</code> version 2.1.15, <span class="math display">\[
f(\overline{p}_{\boldsymbol{v}})=\left\{\mathop{\mathrm{arg\,max}}_{u_i}
\overline{p}_{\boldsymbol{v}}(u_i|\boldsymbol{y}),\,i=1,\dots,n\right\},
\]</span> was used, which caused problems for some nonlinear models.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
